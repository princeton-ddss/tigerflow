{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TigerFlow","text":"<p>TigerFlow is a Python framework that simplifies the creation and execution of data pipelines on Slurm-managed HPC clusters. It supports data pipelines where:</p> <ul> <li>Each task performs embarrassingly parallel file processing. That is, files are processed independently of one another.</li> <li>The task dependency graph forms a rooted tree. That is, the graph has a single root task, and every other task has exactly one parent.</li> </ul> <p>Designed as a continuously running service with dynamic scaling, TigerFlow minimizes the need for users to manually plan and allocate resources in advance.</p>"},{"location":"#why-tigerflow-matters","title":"Why TigerFlow Matters","text":"<p>HPC clusters are an invaluable asset for researchers who require significant computational resources. For example, computational social scientists may need to extract features (e.g., transcription embeddings) from a large volume of TikTok videos and store them in databases for downstream analysis and modeling. However, the architecture of HPC clusters can present challenges for such workflows:</p> <ul> <li> <p>Compute nodes often lack internet access. This prevents direct access to external APIs (e.g., LLM services provided by Google) or remote data sources (e.g., Amazon S3), requiring such tasks to be executed on a login or head node instead.</p> </li> <li> <p>Compute nodes often have restricted access to file systems. Certain file systems (e.g., cold storage) may not be mounted on compute nodes. This necessitates moving or copying data to accessible locations (e.g., scratch space) before processing can occur on compute nodes.</p> </li> </ul> <p>These constraints make it difficult to design and implement end-to-end data pipelines, especially when some steps require external API calls (restricted to login/head nodes) while others depend on high-performance compute resources (available only on compute nodes). TigerFlow addresses these challenges by offering a simple, unified framework for defining and running data pipelines across different types of cluster nodes.</p>"},{"location":"#additional-advantages","title":"Additional Advantages","text":"<p>TigerFlow further streamlines HPC workflows by addressing common inefficiencies in traditional Slurm-based job scheduling:</p> <ul> <li>No need to pre-batch workloads. Each Slurm task in TigerFlow runs a dynamically scalable worker cluster that automatically adapts to the incoming workload, eliminating the need for manual batch planning and tuning.</li> <li>No need to start a new Slurm job for each file. In TigerFlow, a single Slurm job runs as a long-lived worker process that handles multiple files. It performs common operations (e.g., setup and teardown) only once, while applying the actual file-processing logic individually to each file. This reduces idle time and resource waste from launching a separate Slurm job for every file.</li> <li>No need to wait for all files to complete a pipeline step. In TigerFlow, files are processed individually as they arrive, supporting more flexible and dynamic workflows.</li> </ul> <p>These features make TigerFlow especially well-suited for running large-scale or real-time data pipelines on HPC systems.</p>"},{"location":"#how-to-use-tigerflow","title":"How to Use TigerFlow","text":"<p>TigerFlow can be run on any HPC cluster managed by Slurm. Since it is written in Python, the system must have Python (version 3.10 or higher) installed.</p>"},{"location":"#installation","title":"Installation","text":"<p>TigerFlow can be installed using <code>pip</code> or other package managers such as <code>uv</code> and <code>poetry</code>.</p> pipuvpoetry <pre><code>pip install tigerflow\n</code></pre> <pre><code>uv add tigerflow\n</code></pre> <pre><code>poetry add tigerflow\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Once the package is installed, <code>tigerflow</code> command will be available, like so:</p> CommandOutput <pre><code>tigerflow --help\n</code></pre> <pre><code>Usage: tigerflow [OPTIONS] COMMAND [ARGS]...\n\nA pipeline framework optimized for HPC with Slurm integration.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version                                                                   \u2502\n\u2502 --install-completion          Install completion for the current shell.     \u2502\n\u2502 --show-completion             Show completion for the current shell, to     \u2502\n\u2502                               copy it or customize the installation.        \u2502\n\u2502 --help                        Show this message and exit.                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 run      Run a pipeline based on the given specification.                   \u2502\n\u2502 report   Report different types of information about the given pipeline.    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Running the above will display an overview of the tool, including supported subcommands.</p> <p>For instance, <code>run</code> is a subcommand for running a user-defined pipeline, and its details can be viewed by running:</p> CommandOutput <pre><code>tigerflow run --help\n</code></pre> <pre><code>Usage: tigerflow run [OPTIONS] CONFIG_FILE INPUT_DIR OUTPUT_DIR\n\nRun a pipeline based on the given specification.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    config_file      PATH  Configuration file [required]                                        \u2502\n\u2502 *    input_dir        PATH  Directory containing input data for the pipeline [required]          \u2502\n\u2502 *    output_dir       PATH  Directory for storing pipeline outputs and internal data [required]  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --idle-timeout        INTEGER  Terminate after this many minutes of inactivity. [default: 10]    \u2502\n\u2502 --delete-input                 Delete input files after pipeline processing.                     \u2502\n\u2502 --help                         Show this message and exit.                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"#what-next","title":"What Next","text":"<p>Please check out user guides for more detailed instructions and examples.</p>"},{"location":"guides/pipeline/","title":"Pipeline in TigerFlow","text":"<p>Note</p> <p>Before proceeding, please review how to create and use tasks in TigerFlow.</p> <p>In TigerFlow, tasks are organized into a pipeline by creating a configuration file.</p> <p>Let's build on the example from the Task in TigerFlow section, where we created a sequence of tasks to:</p> <ol> <li>Transcribe videos using an open-source model (Whisper)</li> <li>Embed the transcriptions using an external API service (Voyage AI)</li> <li>Ingest the embeddings into a single-writer database (DuckDB)</li> </ol>"},{"location":"guides/pipeline/#defining-a-pipeline","title":"Defining a pipeline","text":"<p>A pipeline is configured using a YAML file. For example, the tasks above can be structured into a pipeline as follows:</p> config.yaml<pre><code>tasks:\n  - name: transcribe\n    kind: slurm\n    module: ./transcribe.py\n    input_ext: .mp4\n    output_ext: .txt\n    resources:\n      cpus: 1\n      gpus: 1\n      memory: \"8G\"\n      time: \"02:00:00\"\n      max_workers: 3\n    setup_commands: |\n      module purge\n      module load anaconda3/2024.6\n      conda activate tiktok\n  - name: embed\n    depends_on: transcribe\n    kind: local_async\n    module: ./embed.py\n    input_ext: .txt\n    output_ext: .json\n    keep_output: false\n    concurrency_limit: 10\n    setup_commands: |\n      module purge\n      module load anaconda3/2024.6\n      conda activate tiktok\n  - name: ingest\n    depends_on: embed\n    kind: local\n    module: ./ingest.py\n    input_ext: .json\n    keep_output: false\n    setup_commands: |\n      module purge\n      module load anaconda3/2024.6\n      conda activate tiktok\n</code></pre> <p>where:</p> <ul> <li><code>kind</code> specifies the task type (one of: <code>local</code>, <code>local_async</code>, or <code>slurm</code>).</li> <li><code>module</code> specifies the Python script defining task logic. Care should be taken when using a relative file path as it may resolve incorrectly when running the pipeline.</li> <li><code>depends_on</code> specifies the name of the parent task whose output is used as input for the current task.</li> <li><code>keep_output</code> specifies whether to retain output files from the current task. If unspecified, it defaults to <code>true</code>.</li> <li><code>setup_commands</code> specifies Bash commands to run before starting the task. This can be used to activate a virtual environment required for the task logic.</li> <li><code>resources</code> is a section applicable only to Slurm tasks. It specifies compute, memory, and other resources to allocate for running the current task. <code>max_workers</code> specifies the maximum number of parallel workers used for auto-scaling.</li> <li><code>concurrency_limit</code> is a field applicable only to local asynchronous tasks. It specifies the maximum number of coroutines (e.g., API requests) that may run concurrently at any given time (excess coroutines are queued until capacity becomes available).</li> </ul> <p>Note</p> <p>TigerFlow supports pipelines where the task dependency graph forms a rooted tree. That is, there must be a single root task, and every other task must have exactly one parent.</p>"},{"location":"guides/pipeline/#running-a-pipeline","title":"Running a pipeline","text":"<p>Assuming the configuration file and task scripts are in the current directory, we can run the pipeline as follows:</p> CommandOutput <pre><code>tigerflow run config.yaml path/to/data/ path/to/results/\n</code></pre> <pre><code>2025-09-22 09:20:10 | INFO     | Starting pipeline execution\n2025-09-22 09:20:10 | INFO     | [transcribe] Starting as a SLURM task\n2025-09-22 09:20:10 | INFO     | [transcribe] Submitted with Slurm job ID 847632\n2025-09-22 09:20:10 | INFO     | [embed] Starting as a LOCAL_ASYNC task\n2025-09-22 09:20:10 | INFO     | [embed] Started with PID 3007442\n2025-09-22 09:20:10 | INFO     | [ingest] Starting as a LOCAL task\n2025-09-22 09:20:10 | INFO     | [ingest] Started with PID 3007443\n2025-09-22 09:20:10 | INFO     | All tasks started, beginning pipeline tracking loop\n2025-09-22 09:20:10 | INFO     | [transcribe] Status changed: INACTIVE -&gt; PENDING (Reason: (None))\n2025-09-22 09:20:10 | INFO     | [embed] Status changed: INACTIVE -&gt; ACTIVE\n2025-09-22 09:20:10 | INFO     | [ingest] Status changed: INACTIVE -&gt; ACTIVE\n2025-09-22 09:20:11 | INFO     | Staged 91 new file(s) for processing\n2025-09-22 09:20:31 | INFO     | [transcribe] Status changed: PENDING (Reason: (None)) -&gt; ACTIVE (0 workers)\n2025-09-22 09:21:01 | INFO     | [transcribe] Status changed: ACTIVE (0 workers) -&gt; ACTIVE (3 workers)\n2025-09-22 09:21:54 | ERROR    | [embed] 4 failed file(s)\n2025-09-22 09:21:55 | INFO     | Completed processing 25 file(s)\n2025-09-22 09:22:05 | ERROR    | [embed] 1 failed file(s)\n2025-09-22 09:22:05 | INFO     | Completed processing 7 file(s)\n2025-09-22 09:22:15 | ERROR    | [embed] 1 failed file(s)\n2025-09-22 09:22:15 | INFO     | Completed processing 13 file(s)\n2025-09-22 09:22:25 | INFO     | Completed processing 11 file(s)\n2025-09-22 09:22:35 | INFO     | Completed processing 3 file(s)\n2025-09-22 09:22:45 | INFO     | Completed processing 5 file(s)\n2025-09-22 09:22:55 | ERROR    | [embed] 1 failed file(s)\n2025-09-22 09:22:55 | INFO     | Completed processing 8 file(s)\n2025-09-22 09:23:05 | ERROR    | [embed] 1 failed file(s)\n2025-09-22 09:23:05 | INFO     | Completed processing 4 file(s)\n2025-09-22 09:23:15 | INFO     | Completed processing 6 file(s)\n2025-09-22 09:23:55 | INFO     | Completed processing 1 file(s)\n2025-09-22 09:25:06 | INFO     | [transcribe] Status changed: ACTIVE (3 workers) -&gt; ACTIVE (1 workers)\n2025-09-22 09:25:46 | INFO     | [transcribe] Status changed: ACTIVE (1 workers) -&gt; ACTIVE (0 workers)\n2025-09-22 09:33:48 | WARNING  | Idle timeout reached, initiating shutdown\n2025-09-22 09:33:48 | INFO     | Shutting down pipeline\n2025-09-22 09:33:48 | INFO     | [embed] Terminating...\n2025-09-22 09:33:48 | INFO     | [ingest] Terminating...\n2025-09-22 09:33:48 | INFO     | [transcribe] Terminating...\n2025-09-22 09:33:49 | ERROR    | [transcribe] Status changed: ACTIVE (0 workers) -&gt; INACTIVE (Reason: CANCELLED+)\n2025-09-22 09:33:50 | ERROR    | [embed] Status changed: ACTIVE -&gt; INACTIVE (Exit Code: 143)\n2025-09-22 09:33:50 | ERROR    | [ingest] Status changed: ACTIVE -&gt; INACTIVE (Exit Code: 143)\n2025-09-22 09:33:51 | INFO     | Pipeline shutdown complete\n</code></pre> <p>Tip</p> <p>Run each task individually (see examples) to ensure they work correctly before executing the entire pipeline.</p> <p>The console output shows that the pipeline:</p> <ul> <li>Runs like a server, \"listening\" for and staging new files for processing</li> <li>Acts as a central orchestrator that launches, monitors, and manages the lifecycle of tasks</li> <li>Optimizes resource usage through autoscaling and idle timeout</li> </ul> <p>By default, pipelines time out after 10 minutes of inactivity (i.e., when there are no more files left to process). We can override this behavior using the <code>--idle-timeout</code> option, like so:</p> <pre><code># Time out after 30 days of inactivity\ntigerflow run config.yaml path/to/data/ path/to/results/ --idle-timeout 43200\n</code></pre> <p>Before the timeout threshold is reached, the pipeline will remain active with a minimal resource footprint, ready to stage and process any new files placed in the input directory. This behavior is useful for streaming-like workflows where data may arrive sporadically.</p> <p>Info</p> <p>To see all available options for the <code>run</code> subcommand, run <code>tigerflow run --help</code>.</p> <p>Since the pipeline has been configured to retain output files only for the transcription task, the output directory (i.e., <code>path/to/results/</code>) will look as follows:</p> <pre><code>path/to/results/\n\u251c\u2500\u2500 .tigerflow/\n\u2514\u2500\u2500 transcribe/\n    \u251c\u2500\u2500 1.txt\n    \u251c\u2500\u2500 2.txt\n    \u2514\u2500\u2500 ...\n</code></pre> <p>where <code>.tigerflow/</code> is an internal directory storing the pipeline's operational state and related metadata.</p> <p>Warning</p> <p><code>.tigerflow/</code> is what enables resuming a previous pipeline run, so it should not be deleted or modified.</p>"},{"location":"guides/pipeline/#checking-progress","title":"Checking progress","text":"<p>We can check the pipeline's progress at any point by running:</p> CommandOutput <pre><code>tigerflow report progress path/to/results/\n</code></pre> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Task       \u2503 Processed \u2503 Ongoing \u2503 Failed \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 transcribe \u2502        91 \u2502       0 \u2502      0 \u2502\n\u2502 embed      \u2502        83 \u2502       0 \u2502      8 \u2502\n\u2502 ingest     \u2502        83 \u2502       0 \u2502      0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nCOMPLETED: \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591 83/91 ( 91.2%)\n</code></pre> <p>where <code>path/to/results/</code> must be a valid output directory containing <code>.tigerflow/</code>.</p>"},{"location":"guides/pipeline/#checking-errors","title":"Checking errors","text":"<p>If the progress reports any failed files, we can identify them by running:</p> CommandOutput <pre><code>tigerflow report errors path/to/results/\n</code></pre> <pre><code>[embed] 8 failed files (open to view errors):\n  results/.tigerflow/embed/7501863358941940997.err\n  results/.tigerflow/embed/7501867598829702430.err\n  results/.tigerflow/embed/7501869468910423326.err\n  results/.tigerflow/embed/7501869707121757470.err\n  results/.tigerflow/embed/7501870655906860306.err\n  results/.tigerflow/embed/7501870694288985390.err\n  results/.tigerflow/embed/7501870878855154987.err\n  results/.tigerflow/embed/7501870943883545899.err\n</code></pre> <p>Each error file contains specific error messages that help identify and resolve issues in the code or data.</p> <p>Example</p> <p>In this case, all error files contain the same message:</p> <pre><code>Traceback (most recent call last):\nFile \"/home/sp8538/.conda/envs/tiktok/lib/python3.12/site-packages/tigerflow/tasks/local_async.py\", line 47, in task\n    await self.run(self._context, input_file, temp_file)\nFile \"/home/sp8538/tiktok/pipeline/tigerflow/demo/code/embed.py\", line 35, in run\n    resp.raise_for_status()  # Raise error if unsuccessful\n    ^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/sp8538/.conda/envs/tiktok/lib/python3.12/site-packages/aiohttp/client_reqrep.py\", line 629, in raise_for_status\n    raise ClientResponseError(\naiohttp.client_exceptions.ClientResponseError: 400, message='Bad Request', url='https://api.voyageai.com/v1/embeddings'\n</code></pre> <p>which suggests an issue with the embedding API request. However, since the same request was successful for other files, the issue likely lies in the input data (i.e., transcription).</p> <p>Upon inspection, we find the failed files have empty transcriptions, which explains the API request failure. Furthermore, we can confirm that the corresponding videos contain no audio, which led to the empty transcriptions in the first place.</p> <p>We may then exclude such videos from the pipeline to prevent future errors.</p>"},{"location":"guides/task/","title":"Task in TigerFlow","text":""},{"location":"guides/task/#overview","title":"Overview","text":"<p>TigerFlow supports three types of tasks:</p> <ul> <li><code>LocalTask</code>: Runs synchronous operations on a login/head node</li> <li><code>LocalAsyncTask</code>: Runs asynchronous operations on a login/head node</li> <li><code>SlurmTask</code>: Runs parallel operations across compute nodes via Slurm</li> </ul> <p>To define a task, subclass one of these types and implement the following methods:</p> Method Required Description <code>setup</code> No Initializes shared context used across multiple files (e.g., loading a model) <code>run</code> Yes Contains the processing logic applied to each file <code>teardown</code> No Performs cleanup operations for graceful shutdown (e.g., closing a database connection) <p>Then, simply call the inherited <code>cli()</code> method to turn the module into a runnable CLI application.</p> <p>For instance, we can define a simple local task that converts text files to uppercase as follows:</p> upper.py<pre><code>from tigerflow.tasks import LocalTask\n\n\nclass Upper(LocalTask):\n    @staticmethod\n    def setup(context):\n        context.common_data = \"Common Data from Setup\"\n        print(\"Setup executed successfully!\")\n\n    @staticmethod\n    def run(context, input_file, output_file):\n        with open(input_file, \"r\") as fi:\n            content = fi.read()\n\n        new_content = context.common_data + \"\\n\" + content.upper()\n\n        with open(output_file, \"w\") as fo:\n            fo.write(new_content)\n\n    @staticmethod\n    def teardown(context):\n        print(\"Teardown executed successfully!\")\n\n\nUpper.cli()\n</code></pre> <p>where:</p> <ul> <li><code>context</code> is a namespace to store and access any common, reusable data/objects (e.g., DB connection)</li> <li><code>input_file</code> is a path to the input file to be processed</li> <li><code>output_file</code> is a path to the output file to be generated</li> </ul> <p>With <code>Upper.cli()</code>, this module becomes a runnable CLI application and we can check its details by running:</p> CommandOutput <pre><code>python upper.py --help\n</code></pre> <pre><code>Usage: test.py [OPTIONS]\n\nRun the task as a CLI application\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-dir         PATH  Input directory to read data [required]        \u2502\n\u2502 *  --input-ext         TEXT  Input file extension [required]                \u2502\n\u2502 *  --output-dir        PATH  Output directory to store results [required]   \u2502\n\u2502 *  --output-ext        TEXT  Output file extension [required]               \u2502\n\u2502    --help                    Show this message and exit.                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>We can then run the task as follows:</p> CommandOutput <pre><code>python test.py --input-dir path/to/data/ --input-ext .txt --output-dir path/to/results/ --output-ext .txt\n</code></pre> <pre><code>2025-09-11 10:54:23 | INFO     | Setting up task\nSetup executed successfully!\n2025-09-11 10:54:23 | INFO     | Task setup complete\n2025-09-11 10:54:23 | INFO     | Starting processing: 5.txt\n2025-09-11 10:54:23 | INFO     | Successfully processed: 5.txt\n2025-09-11 10:54:23 | INFO     | Starting processing: 4.txt\n2025-09-11 10:54:23 | INFO     | Successfully processed: 4.txt\n...\n2025-09-11 10:54:23 | INFO     | Starting processing: 1.txt\n2025-09-11 10:54:23 | INFO     | Successfully processed: 1.txt\n^C2025-09-11 10:54:30 | WARNING  | Received signal 2, initiating shutdown\n2025-09-11 10:54:30 | INFO     | Shutting down task\nTeardown executed successfully!\n2025-09-11 10:54:30 | INFO     | Task shutdown complete\n</code></pre> <p>Info</p> <p>If a file is not processed successfully, an error output file will be generated (e.g., <code>4.err</code> instead of <code>4.txt</code>). This file contains specific error messages to assist with debugging.</p>"},{"location":"guides/task/#examples","title":"Examples","text":"<p>Say we want to implement the following workflow:</p> <ol> <li>Transcribe video files using an open-source model (e.g., Whisper)</li> <li>Embed the transcription files using an external API service (e.g., Voyage AI)</li> <li>Ingest the embeddings into a single-writer database (e.g., DuckDB)</li> </ol> <p>We can create and test each task as shown below.</p>"},{"location":"guides/task/#transcribing-video-files-slurmtask","title":"Transcribing Video Files (<code>SlurmTask</code>)","text":"<p>We implement the transcription step as a Slurm task because it involves compute-intensive work and we want to process files in parallel.</p> transcribe.py<pre><code>import whisper\n\nfrom tigerflow.tasks import SlurmTask\n\n\nclass Transcribe(SlurmTask):\n    @staticmethod\n    def setup(context):\n        context.model = whisper.load_model(\n            \"medium\",\n            download_root=\"/home/sp8538/.cache/whisper\",\n        )\n        print(\"Model loaded successfully\")\n\n    @staticmethod\n    def run(context, input_file, output_file):\n        result = context.model.transcribe(str(input_file))\n        print(f\"Transcription ran successfully for {input_file}\")\n\n        with open(output_file, \"w\") as f:\n            f.write(result[\"text\"])\n\n\nTranscribe.cli()\n</code></pre> <p>As shown, the task is defined such that:</p> <ul> <li>The model is loaded once during setup and stored in <code>context</code></li> <li>This pre-loaded model is then accessed from <code>context</code> to transcribe each file</li> </ul> <p>Warning</p> <p>With</p> <pre><code>context.model = whisper.load_model(\n    \"medium\",\n    download_root=\"/home/sp8538/.cache/whisper\",\n)\n</code></pre> <p>Whisper will attempt to load the model from <code>download_root</code> if the model file (<code>medium.pt</code>) is already present. If the file is missing, it will try to download it, which would fail in this case because <code>SlurmTask</code> runs on compute nodes without internet access.</p> <p>To avoid this issue, we can update the code to explicitly load the model from a local path:</p> <pre><code>local_model_path = \"/home/sp8538/.cache/whisper/medium.pt\"\nmodel = torch.load(local_model_path)\ncontext.model = whisper.Whisper(model)\n</code></pre> <p>Calling <code>Transcribe.cli()</code> turns this module into a runnable CLI application:</p> CommandOutput <pre><code>python transcribe.py --help\n</code></pre> <pre><code>Usage: transcribe.py [OPTIONS]\n\nRun the task as a CLI application\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-dir             PATH     Input directory to read data [required]            \u2502\n\u2502 *  --input-ext             TEXT     Input file extension [required]                    \u2502\n\u2502 *  --output-dir            PATH     Output directory to store results [required]       \u2502\n\u2502 *  --output-ext            TEXT     Output file extension [required]                   \u2502\n\u2502 *  --cpus                  INTEGER  Number of CPUs per worker [required]               \u2502\n\u2502 *  --memory                TEXT     Memory per worker [required]                       \u2502\n\u2502 *  --time                  TEXT     Wall time per worker [required]                    \u2502\n\u2502 *  --max-workers           INTEGER  Max number of workers for autoscaling [required]   \u2502\n\u2502    --gpus                  INTEGER  Number of GPUs per worker                          \u2502\n\u2502    --setup-commands        TEXT     Shell commands to run before the task starts       \u2502\n\u2502                                     (separate commands with a semicolon)               \u2502\n\u2502    --task-name             TEXT     Task name [default: Transcribe]                    \u2502\n\u2502    --help                           Show this message and exit.                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>We can then run the task as follows:</p> CommandOutput <pre><code>python transcribe.py \\\n--input-dir path/to/data/ \\\n--input-ext .mp4 \\\n--output-dir path/to/results/ \\\n--output-ext .txt \\\n--cpus 1 \\\n--memory \"12G\" \\\n--time \"02:00:00\" \\\n--max-workers 3 \\\n--gpus 1 \\\n--setup-commands \"module purge; module load anaconda3/2024.6; conda activate tiktok\"\n</code></pre> <pre><code>2025-09-16 10:53:44 | INFO     | Submitted task with Slurm job ID 690468\n2025-09-16 10:53:44 | INFO     | Status changed: INACTIVE -&gt; PENDING (Reason: (None))\n2025-09-16 10:54:04 | INFO     | Status changed: PENDING (Reason: (None)) -&gt; ACTIVE (0 workers)\n2025-09-16 10:59:55 | INFO     | Status changed: ACTIVE (0 workers) -&gt; ACTIVE (1 workers)\n2025-09-16 11:00:45 | INFO     | 4 processed files\n2025-09-16 11:00:55 | INFO     | Status changed: ACTIVE (1 workers) -&gt; ACTIVE (3 workers)\n2025-09-16 11:00:55 | INFO     | 1 processed files\n2025-09-16 11:01:05 | INFO     | 6 processed files\n...\n2025-09-16 11:03:08 | INFO     | 2 processed files\n2025-09-16 11:04:08 | INFO     | 1 processed files\n2025-09-16 11:04:58 | INFO     | Status changed: ACTIVE (3 workers) -&gt; ACTIVE (1 workers)\n2025-09-16 11:05:58 | INFO     | Status changed: ACTIVE (1 workers) -&gt; ACTIVE (0 workers)\n^C2025-09-16 11:06:40 | WARNING  | Received signal 2, initiating shutdown\n2025-09-16 11:06:40 | INFO     | Shutting down task\n2025-09-16 11:06:40 | ERROR    | Status changed: ACTIVE (0 workers) -&gt; INACTIVE (Reason: CANCELLED+)\n2025-09-16 11:06:41 | INFO     | Task shutdown complete\n</code></pre> <p>Note</p> <p>The resources specified here, including <code>time</code>, apply to each individual worker. Workers can be spun up and down dynamically in response to incoming workloads, so it is beneficial to allocate only the minimal necessary resources per worker.</p> <p>For example, setting the worker <code>time</code> to a reasonable value like 2 hours (instead of 12 hours) can reduce scheduling delays, as longer Slurm job requests often result in longer queue times. Of course, the definition of \"reasonable\" depends on the nature of the work the worker performs. For instance, if processing each file takes around 3 hours, setting the worker <code>time</code> to 12 hours may be appropriate.</p>"},{"location":"guides/task/#embedding-text-files-localasynctask","title":"Embedding Text Files (<code>LocalAsyncTask</code>)","text":"<p>We implement the embedding step as a local asynchronous task because it involves I/O-bound work (i.e., making external API requests) and we want to process multiple files concurrently.</p> embed.py<pre><code>import asyncio\nimport os\n\nimport aiofiles\nimport aiohttp\n\nfrom tigerflow.tasks import LocalAsyncTask\n\n\nclass Embed(LocalAsyncTask):\n    @staticmethod\n    async def setup(context):\n        context.url = \"https://api.voyageai.com/v1/embeddings\"\n        context.headers = {\n            \"Authorization\": f\"Bearer {os.environ['VOYAGE_API_KEY']}\",\n            \"Content-Type\": \"application/json\",\n        }\n        context.session = aiohttp.ClientSession()\n        print(\"Session created successfully!\")\n\n    @staticmethod\n    async def run(context, input_file, output_file):\n        async with aiofiles.open(input_file, \"r\") as f:\n            text = await f.read()\n\n        async with context.session.post(\n            context.url,\n            headers=context.headers,\n            json={\n                \"input\": text.strip(),\n                \"model\": \"voyage-3.5\",\n                \"input_type\": \"document\",\n            },\n        ) as resp:\n            resp.raise_for_status()  # Raise error if unsuccessful\n            result = await resp.text()  # Raw JSON\n            await asyncio.sleep(1)  # For API rate limit\n\n        async with aiofiles.open(output_file, \"w\") as f:\n            await f.write(result)\n\n    @staticmethod\n    async def teardown(context):\n        await context.session.close()\n        print(\"Session closed successfully!\")\n\n\nEmbed.cli()\n</code></pre> <p>As shown, the task is defined such that it:</p> <ul> <li>Initializes reusable resources (e.g., HTTP session) and stores them in <code>context</code></li> <li>Utilizes these resources from <code>context</code> to send a request to the external API for each input file</li> <li>Cleans up resources (e.g., HTTP session) at the end to ensure a graceful shutdown</li> </ul> <p>Info</p> <p><code>LocalAsyncTask</code> requires all operations to adhere to Python's <code>async</code>/<code>await</code> syntax. For example, as shown above, file reading and writing are performed using <code>aiofiles</code>, since standard file I/O would block the event loop and prevent concurrent file processing. Similarly, <code>LocalAsyncTask</code> should not include compute-intensive logic, as this would also block the event loop and goes against its intended use. For compute-heavy tasks, consider using <code>SlurmTask</code> instead.</p> API Rate Limits <p>API services often enforce rate limits (e.g., 2000 requests per minute). To comply with these limits, we can use <code>asyncio.sleep()</code> within the <code>run</code> logic (as shown above), in combination with the <code>--concurrency-limit</code> option (see below), which controls the maximum number of files processed concurrently.</p> <p>For example, if each API request takes less than a second and the service allows up to 2000 requests per minute, we can:</p> <ul> <li>Use <code>asyncio.sleep(1)</code> in the <code>run</code> logic to ensure each request takes at least one second</li> <li>Set <code>--concurrency-limit</code> to 30 to ensure no more than 30 requests are processed concurrently</li> </ul> <p>Together, these measures effectively cap the request rate at 1800 requests per minute, keeping it safely within the limit.</p> <p>Calling <code>Embed.cli()</code> turns this module into a runnable CLI application:</p> CommandOutput <pre><code>python embed.py --help\n</code></pre> <pre><code>Usage: embed.py [OPTIONS]\n\nRun the task as a CLI application\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-dir                PATH     Input directory to read data [required]                                 \u2502\n\u2502 *  --input-ext                TEXT     Input file extension [required]                                         \u2502\n\u2502 *  --output-dir               PATH     Output directory to store results [required]                            \u2502\n\u2502 *  --output-ext               TEXT     Output file extension [required]                                        \u2502\n\u2502 *  --concurrency-limit        INTEGER  Maximum number of coroutines that may run concurrently at any given     \u2502\n\u2502                                        time (excess coroutines are queued until capacity becomes available)    \u2502\n\u2502                                        [required]                                                              \u2502\n\u2502    --task-name                TEXT     Task name [default: Embed]                                              \u2502\n\u2502    --help                              Show this message and exit.                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>We can then run the task as follows:</p> CommandOutput <pre><code>python embed.py \\\n--input-dir path/to/data/ \\\n--input-ext .txt \\\n--output-dir path/to/results/ \\\n--output-ext .json \\\n--concurrency-limit 30\n</code></pre> <pre><code>2025-09-19 10:28:32 | INFO     | Setting up task\nSession created successfully!\n2025-09-19 10:28:32 | INFO     | Task setup complete\n2025-09-19 10:28:32 | INFO     | Starting processing: 7501870786337115434.txt\n2025-09-19 10:28:32 | INFO     | Starting processing: 7501870786941127967.txt\n2025-09-19 10:28:32 | INFO     | Starting processing: 7501870783862541576.txt\n...\n2025-09-19 10:28:34 | INFO     | Starting processing: 7501869901028592927.txt\n2025-09-19 10:28:34 | INFO     | Successfully processed: 7501871089715268906.txt\n2025-09-19 10:28:34 | INFO     | Starting processing: 7501870443775692078.txt\n2025-09-19 10:28:34 | INFO     | Successfully processed: 7501863546456771870.txt\n2025-09-19 10:28:34 | INFO     | Starting processing: 7501869899782901022.txt\n2025-09-19 10:28:34 | INFO     | Successfully processed: 7501870775461317906.txt\n...\n2025-09-19 10:28:36 | INFO     | Successfully processed: 7501870542656474398.txt\n2025-09-19 10:28:36 | INFO     | Successfully processed: 7501870861306318126.txt\n2025-09-19 10:28:36 | INFO     | Successfully processed: 7501870700089707807.txt\n^C2025-09-19 10:28:40 | WARNING  | Received signal 2, initiating shutdown\n2025-09-19 10:28:40 | INFO     | Shutting down task\nSession closed successfully!\n2025-09-19 10:28:40 | INFO     | Task shutdown complete\n</code></pre>"},{"location":"guides/task/#ingesting-text-embeddings-localtask","title":"Ingesting Text Embeddings (<code>LocalTask</code>)","text":"<p>We implement the ingestion step as a local synchronous task because our target database (DuckDB) only supports writes from a single process.</p> ingest.py<pre><code>import json\n\nimport duckdb\n\nfrom tigerflow.tasks import LocalTask\n\n\nclass Ingest(LocalTask):\n    @staticmethod\n    def setup(context):\n        db_path = \"/home/sp8538/tiktok/pipeline/tigerflow/demo/results/test.db\"\n\n        conn = duckdb.connect(db_path)  # Creates file if not existing\n        print(f\"Successfully connected to {db_path}\")\n\n        conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS embeddings (\n                id UBIGINT,\n                embedding FLOAT[1024],\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            );\n        \"\"\")\n\n        context.conn = conn\n\n    @staticmethod\n    def run(context, input_file, output_file):\n        with open(input_file, \"r\") as f:\n            content = json.load(f)\n\n        embedding = content[\"data\"][0][\"embedding\"]\n\n        context.conn.execute(\n            \"INSERT INTO embeddings (id, embedding) VALUES (?, ?)\",\n            (input_file.stem, embedding),\n        )\n\n    @staticmethod\n    def teardown(context):\n        context.conn.close()\n        print(\"DB connection closed\")\n\n\nIngest.cli()\n</code></pre> <p>As shown, the task is defined such that:</p> <ul> <li>A database connection is created once during setup and stored in <code>context</code></li> <li>This database connection is then accessed from <code>context</code> to ingest each file</li> <li>The database connection is closed at the end to ensure a graceful shutdown</li> </ul> <p>Calling <code>Ingest.cli()</code> turns this module into a runnable CLI application:</p> CommandOutput <pre><code>python ingest.py --help\n</code></pre> <pre><code>Usage: ingest.py [OPTIONS]\n\nRun the task as a CLI application\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-dir         PATH  Input directory to read data [required]         \u2502\n\u2502 *  --input-ext         TEXT  Input file extension [required]                 \u2502\n\u2502 *  --output-dir        PATH  Output directory to store results [required]    \u2502\n\u2502 *  --output-ext        TEXT  Output file extension [required]                \u2502\n\u2502    --task-name         TEXT  Task name [default: Ingest]                     \u2502\n\u2502    --help                    Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>We can then run the task as follows:</p> CommandOutput <pre><code>python ingest.py \\\n--input-dir path/to/data/ \\\n--input-ext .json \\\n--output-dir path/to/results/ \\\n--output-ext .out\n</code></pre> <pre><code>2025-09-19 13:02:16 | INFO     | Task setup complete\n2025-09-19 13:02:16 | INFO     | Starting processing: 7501869531975929119.json\n2025-09-19 13:02:17 | INFO     | Successfully processed: 7501869531975929119.json\n2025-09-19 13:02:17 | INFO     | Starting processing: 7501869470705782062.json\n2025-09-19 13:02:17 | INFO     | Successfully processed: 7501869470705782062.json\n2025-09-19 13:02:17 | INFO     | Starting processing: 7501871439457373470.json\n...\n2025-09-19 13:02:18 | INFO     | Successfully processed: 7501870861306318126.json\n2025-09-19 13:02:18 | INFO     | Starting processing: 7501870700089707807.json\n2025-09-19 13:02:18 | INFO     | Successfully processed: 7501870700089707807.json\n^C2025-09-19 13:02:22 | WARNING  | Received signal 2, initiating shutdown\n2025-09-19 13:02:22 | INFO     | Shutting down task\nDB connection closed\n2025-09-19 13:02:22 | INFO     | Task shutdown complete\n</code></pre> <p>Info</p> <p>Note that we specify <code>--output-dir</code> and <code>--output-ext</code> even though the task\u2019s <code>run</code> logic does not write to <code>output_file</code>. This is necessary because TigerFlow creates an empty \"placeholder\" file even when no content is written. This placeholder indicates that the file was processed successfully according to the user-provided <code>run</code> logic, even if no concrete output was produced. If processing fails, however, TigerFlow generates a separate error file containing the relevant error message instead of the placeholder.</p>"}]}