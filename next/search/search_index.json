{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":"<p>TigerFlow is a Python framework that simplifies the creation and execution of data pipelines on Slurm-managed HPC clusters. It supports data pipelines where:</p> <ul> <li>Each task performs embarrassingly parallel, one-to-one file processing. That is, each input file is transformed into a single output file independently of all other input files.</li> <li>The graph of task input/output files forms an arborescence. That is, there is a single root file, and every other file depends on exactly one parent.</li> </ul> <p>Designed as a continuously running service with dynamic scaling, TigerFlow minimizes the need for users to manually plan and allocate resources in advance.</p>"},{"location":"#why-tigerflow","title":"Why TigerFlow?","text":"<p>HPC clusters are an invaluable asset for researchers who require significant computational resources. For example, computational social scientists may need to extract features (e.g., transcription embeddings) from a large volume of TikTok videos and store them in databases for downstream analysis and modeling. However, the architecture of HPC clusters can present challenges for such workflows:</p> <ul> <li> <p>Compute nodes often lack internet access. This prevents direct access to external APIs (e.g., LLM services provided by Google) or remote data sources (e.g., Amazon S3), requiring such tasks to be executed on a login or head node instead.</p> </li> <li> <p>Compute nodes often have restricted access to file systems. Certain file systems (e.g., cold storage) may not be mounted on compute nodes. This necessitates moving or copying data to accessible locations (e.g., scratch space) before processing can occur on compute nodes.</p> </li> </ul> <p>These constraints make it difficult to design and implement end-to-end data pipelines when some steps require external API call\u2014restricted to login/head nodes\u2014while others depend on high-performance compute resources available only on compute nodes. TigerFlow addresses these challenges by offering a simple, unified framework for defining and running data pipelines across different types of cluster nodes.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>TigerFlow further streamlines HPC workflows by addressing common inefficiencies in traditional Slurm-based job scheduling:</p> <ul> <li>No need to pre-batch workloads. Each Slurm task in TigerFlow runs a dynamically scalable worker cluster that automatically adapts to the incoming workload, eliminating the need for manual batch planning and tuning.</li> <li>No need to start a new Slurm job for each file. In TigerFlow, a single Slurm job runs as a long-lived worker process that handles multiple files. It performs shared operations (e.g., setup and teardown) once, while applying file-processing logic individually to each file. This reduces idle time and resource waste from launching a separate Slurm job for every file.</li> <li>No need to wait for all files to complete a pipeline step. In TigerFlow, files are processed individually as they arrive, supporting more flexible and dynamic workflows.</li> </ul> <p>These features make TigerFlow especially well-suited for running large-scale or real-time data pipelines on HPC systems.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>TigerFlow can be run on any HPC cluster managed by Slurm. Since it is written in Python, the system must have Python (versions 3.10 through 3.13) installed.</p>"},{"location":"#installation","title":"Installation","text":"<p>TigerFlow can be installed using <code>pip</code> or other package managers such as <code>uv</code> and <code>poetry</code>.</p> pipuvpoetry <pre><code>pip install tigerflow\n</code></pre> <pre><code>uv add tigerflow\n</code></pre> <pre><code>poetry add tigerflow\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Once the package is installed, <code>tigerflow</code> command will be available, like so:</p> CommandOutput <pre><code>tigerflow --help\n</code></pre> <pre><code>Usage: tigerflow [OPTIONS] COMMAND [ARGS]...\n\nA pipeline framework optimized for HPC with Slurm integration.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --version                                                                   \u2502\n\u2502 --install-completion          Install completion for the current shell.     \u2502\n\u2502 --show-completion             Show completion for the current shell, to     \u2502\n\u2502                               copy it or customize the installation.        \u2502\n\u2502 --help                        Show this message and exit.                   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 run      Run a pipeline based on the given specification.                   \u2502\n\u2502 report   Report different types of information about the given pipeline.    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Running the above will display an overview of the tool, including supported subcommands.</p> <p>For instance, <code>run</code> is a subcommand for running a user-defined pipeline, and its details can be viewed by running:</p> CommandOutput <pre><code>tigerflow run --help\n</code></pre> <pre><code>Usage: tigerflow run [OPTIONS] CONFIG_FILE INPUT_DIR OUTPUT_DIR\n\nRun a pipeline based on the given specification.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    config_file      PATH  Configuration file [required]                                        \u2502\n\u2502 *    input_dir        PATH  Directory containing input data for the pipeline [required]          \u2502\n\u2502 *    output_dir       PATH  Directory for storing pipeline outputs and internal data [required]  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --idle-timeout        INTEGER  Terminate after this many minutes of inactivity. [default: 10]    \u2502\n\u2502 --delete-input                 Delete input files after pipeline processing.                     \u2502\n\u2502 --help                         Show this message and exit.                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Try running the examples, starting with a simple pipeline consisting of two local tasks.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Check out the user guide for more details.</p>"},{"location":"guides/configuration/","title":"Configuration","text":"<p>TigerFlow provides several configurable settings that control internal timing, polling intervals, and Slurm task behavior. These settings can be customized via environment variables or a <code>.env</code> file.</p>"},{"location":"guides/configuration/#environment-variables","title":"Environment Variables","text":"<p>All settings use the <code>TIGERFLOW_</code> prefix. For example, to set the pipeline polling interval:</p> <pre><code>export TIGERFLOW_PIPELINE_POLL_INTERVAL=30\n</code></pre>"},{"location":"guides/configuration/#using-a-env-file","title":"Using a <code>.env</code> File","text":"<p>You can also define settings in a <code>.env</code> file in your working directory:</p> .env<pre><code>TIGERFLOW_PIPELINE_POLL_INTERVAL=30\nTIGERFLOW_TASK_POLL_INTERVAL=5\nTIGERFLOW_SLURM_TASK_CLIENT_HOURS=12\n</code></pre> <p>To use a custom <code>.env</code> file location, set the <code>TIGERFLOW_ENV_FILE</code> environment variable:</p> <pre><code>export TIGERFLOW_ENV_FILE=/path/to/custom.env\n</code></pre>"},{"location":"guides/configuration/#available-settings","title":"Available Settings","text":"Setting Default Description <code>TIGERFLOW_TASK_VALIDATION_TIMEOUT</code> <code>60</code> Timeout in seconds for validating task modules <code>TIGERFLOW_PIPELINE_POLL_INTERVAL</code> <code>10</code> Pipeline polling interval in seconds <code>TIGERFLOW_TASK_POLL_INTERVAL</code> <code>3</code> Task polling interval in seconds <code>TIGERFLOW_SLURM_TASK_CLIENT_HOURS</code> <code>24</code> Time limit in hours for each Slurm task client job (respawns when expired) <code>TIGERFLOW_SLURM_TASK_SCALE_INTERVAL</code> <code>15</code> Interval in seconds between Slurm task scaling checks <code>TIGERFLOW_SLURM_TASK_SCALE_WAIT_COUNT</code> <code>8</code> Number of consecutive idle checks before removing a worker <code>TIGERFLOW_SLURM_TASK_WORKER_STARTUP_TIMEOUT</code> <code>600</code> Timeout in seconds for each Slurm task worker to initialize"},{"location":"guides/configuration/#example-tuning-slurm-task-behavior","title":"Example: Tuning Slurm Task Behavior","text":"<p>For workflows with bursty workloads, you may want workers to remain available longer before scaling down:</p> .env<pre><code># Check scaling less frequently (every 30 seconds instead of 15)\nTIGERFLOW_SLURM_TASK_SCALE_INTERVAL=30\n\n# Require more idle checks before removing a worker (16 instead of 8)\nTIGERFLOW_SLURM_TASK_SCALE_WAIT_COUNT=16\n</code></pre> <p>For environments with slow Slurm queues or large model loading times, you may need to increase the worker startup timeout:</p> .env<pre><code># Allow longer time for workers to initialize (20 minutes instead of 10)\nTIGERFLOW_SLURM_TASK_WORKER_STARTUP_TIMEOUT=1200\n</code></pre>"},{"location":"guides/pipeline/","title":"Pipelines","text":"<p>Note</p> <p>This guide assumes you are familiar with TigerFlow tasks. Please review how to create and use tasks in TigerFlow before proceeding.</p> <p>In TigerFlow, tasks are organized into a pipeline by creating a configuration file that describes the tasks to be run, the resources required by each task, and the dependencies between tasks. Tasks communicate through the file system: a parent task writes its outputs to a designated directory, which downstream tasks monitor for new inputs.<sup>1</sup> Since each task performs embarrassingly parallel, one-to-one file processing (i.e., each input is transformed into a single output independently of all other inputs), multiple tasks may share the same parent, but each task can have at most one parent.</p> <p>Dependency Graph</p> <p>TigerFlow supports pipelines where the graph of task input/output files forms an arborescence. That is, there is a single root file, and every other file depends on exactly one parent. This means that the pipeline can contain multiple root tasks (i.e., tasks that depend on no other tasks), but they should share the same input file.</p> <p></p> <p>Let's build on the example from the previous section, where we created a sequence of tasks to:</p> <ol> <li>Transcribe videos using an open-source model (Whisper)</li> <li>Embed the transcriptions using an external API service (Voyage AI)</li> <li>Ingest the embeddings into a single-writer database (DuckDB)</li> </ol> <p>Follow Along</p> <p>You can follow along with the example using the code and data provided here. Videos have been substituted with audio files due to intellectual property constraints and storage limitations, but the pipeline remains otherwise identical.</p>"},{"location":"guides/pipeline/#defining-a-pipeline","title":"Defining a Pipeline","text":"<p>A pipeline is configured using a YAML file. For example, the tasks above can be structured into a pipeline as follows:</p> config.yaml<pre><code>tasks:\n  - name: transcribe\n    kind: slurm\n    module: ./transcribe.py\n    input_ext: .mp4\n    output_ext: .txt\n    max_workers: 3\n    worker_resources:\n      cpus: 1\n      gpus: 1\n      memory: 8G\n      time: 02:00:00\n      sbatch_options:\n        - \"--mail-user=sp8538@princeton.edu\"\n    setup_commands:\n      - module purge\n      - module load anaconda3/2024.6\n      - conda activate tiktok\n  - name: embed\n    depends_on: transcribe\n    kind: local_async\n    module: ./embed.py\n    input_ext: .txt\n    output_ext: .json\n    keep_output: false\n    concurrency_limit: 10\n    setup_commands:\n      - module purge\n      - module load anaconda3/2024.6\n      - conda activate tiktok\n  - name: ingest\n    depends_on: embed\n    kind: local\n    module: ./ingest.py\n    input_ext: .json\n    keep_output: false\n    setup_commands:\n      - module purge\n      - module load anaconda3/2024.6\n      - conda activate tiktok\n</code></pre> <p>where:</p> <ul> <li><code>kind</code> specifies the task type (one of: <code>local</code>, <code>local_async</code>, or <code>slurm</code>).</li> <li><code>module</code> specifies the Python script defining task logic. Care should be taken when using a relative file path as it may resolve incorrectly when running the pipeline.</li> <li><code>depends_on</code> specifies the name of the parent task whose output is used as input for the current task.</li> <li><code>keep_output</code> specifies whether to retain output files from the current task. If unspecified, it defaults to <code>true</code>.</li> <li><code>setup_commands</code> specifies a list of Bash commands to run before starting the task. This can be used to activate a virtual environment required for the task logic.</li> <li><code>max_workers</code> is a field applicable only to Slurm tasks. It specifies the maximum number of parallel workers used for auto-scaling.</li> <li><code>worker_resources</code> is a section applicable only to Slurm tasks. It specifies compute, memory, and other resources to allocate for each worker.</li> <li><code>concurrency_limit</code> is a field applicable only to local asynchronous tasks. It specifies the maximum number of coroutines (e.g., API requests) that may run concurrently at any given time (excess coroutines are queued until capacity becomes available).</li> </ul>"},{"location":"guides/pipeline/#running-a-pipeline","title":"Running a Pipeline","text":"<p>Assuming the configuration file and task scripts are in the current directory, we can run the pipeline as follows:</p> CommandOutput <pre><code>tigerflow run config.yaml path/to/data/ path/to/results/\n</code></pre> <pre><code>2025-09-22 09:20:10 | INFO     | Starting pipeline execution\n2025-09-22 09:20:10 | INFO     | [transcribe] Starting as a SLURM task\n2025-09-22 09:20:10 | INFO     | [transcribe] Submitted with Slurm job ID 847632\n2025-09-22 09:20:10 | INFO     | [embed] Starting as a LOCAL_ASYNC task\n2025-09-22 09:20:10 | INFO     | [embed] Started with PID 3007442\n2025-09-22 09:20:10 | INFO     | [ingest] Starting as a LOCAL task\n2025-09-22 09:20:10 | INFO     | [ingest] Started with PID 3007443\n2025-09-22 09:20:10 | INFO     | All tasks started, beginning pipeline tracking loop\n2025-09-22 09:20:10 | INFO     | [transcribe] Status changed: INACTIVE -&gt; PENDING (Reason: (None))\n2025-09-22 09:20:10 | INFO     | [embed] Status changed: INACTIVE -&gt; ACTIVE\n2025-09-22 09:20:10 | INFO     | [ingest] Status changed: INACTIVE -&gt; ACTIVE\n2025-09-22 09:20:11 | INFO     | Staged 91 new file(s) for processing\n2025-09-22 09:20:31 | INFO     | [transcribe] Status changed: PENDING (Reason: (None)) -&gt; ACTIVE (0 workers)\n2025-09-22 09:21:01 | INFO     | [transcribe] Status changed: ACTIVE (0 workers) -&gt; ACTIVE (3 workers)\n2025-09-22 09:21:54 | ERROR    | [embed] 4 failed file(s)\n2025-09-22 09:21:55 | INFO     | Completed processing 25 file(s)\n2025-09-22 09:22:05 | ERROR    | [embed] 1 failed file(s)\n2025-09-22 09:22:05 | INFO     | Completed processing 7 file(s)\n2025-09-22 09:22:15 | ERROR    | [embed] 1 failed file(s)\n2025-09-22 09:22:15 | INFO     | Completed processing 13 file(s)\n2025-09-22 09:22:25 | INFO     | Completed processing 11 file(s)\n2025-09-22 09:22:35 | INFO     | Completed processing 3 file(s)\n2025-09-22 09:22:45 | INFO     | Completed processing 5 file(s)\n2025-09-22 09:22:55 | ERROR    | [embed] 1 failed file(s)\n2025-09-22 09:22:55 | INFO     | Completed processing 8 file(s)\n2025-09-22 09:23:05 | ERROR    | [embed] 1 failed file(s)\n2025-09-22 09:23:05 | INFO     | Completed processing 4 file(s)\n2025-09-22 09:23:15 | INFO     | Completed processing 6 file(s)\n2025-09-22 09:23:55 | INFO     | Completed processing 1 file(s)\n2025-09-22 09:25:06 | INFO     | [transcribe] Status changed: ACTIVE (3 workers) -&gt; ACTIVE (1 workers)\n2025-09-22 09:25:46 | INFO     | [transcribe] Status changed: ACTIVE (1 workers) -&gt; ACTIVE (0 workers)\n2025-09-22 09:33:48 | WARNING  | Idle timeout reached, initiating shutdown\n2025-09-22 09:33:48 | INFO     | Shutting down pipeline\n2025-09-22 09:33:48 | INFO     | [embed] Terminating...\n2025-09-22 09:33:48 | INFO     | [ingest] Terminating...\n2025-09-22 09:33:48 | INFO     | [transcribe] Terminating...\n2025-09-22 09:33:49 | ERROR    | [transcribe] Status changed: ACTIVE (0 workers) -&gt; INACTIVE (Reason: CANCELLED+)\n2025-09-22 09:33:50 | ERROR    | [embed] Status changed: ACTIVE -&gt; INACTIVE (Exit Code: 143)\n2025-09-22 09:33:50 | ERROR    | [ingest] Status changed: ACTIVE -&gt; INACTIVE (Exit Code: 143)\n2025-09-22 09:33:51 | INFO     | Pipeline shutdown complete\n</code></pre> <p>Tip</p> <p>Run each task individually (see examples) to ensure they work correctly before executing the entire pipeline.</p> <p>The console output shows that the pipeline:</p> <ul> <li>Runs like a server, \"listening\" for and staging new files for processing</li> <li>Acts as a central orchestrator that launches, monitors, and manages the lifecycle of tasks</li> <li>Optimizes resource usage through autoscaling and idle timeout</li> </ul> <p>By default, pipelines time out after 10 minutes of inactivity (i.e., when there are no more files left to process). We can override this behavior using the <code>--idle-timeout</code> option, like so:</p> <pre><code># Time out after 30 days of inactivity\ntigerflow run config.yaml path/to/data/ path/to/results/ --idle-timeout 43200\n</code></pre> <p>Before the timeout threshold is reached, the pipeline will remain active with a minimal resource footprint, ready to stage and process any new files placed in the input directory. This behavior is useful for streaming-like workflows where data may arrive sporadically.</p> <p>Info</p> <p>To see all available options for the <code>run</code> subcommand, run <code>tigerflow run --help</code>.</p> <p>Since the pipeline has been configured to retain output files only for the transcription task, the output directory (i.e., <code>path/to/results/</code>) will look as follows:</p> <pre><code>path/to/results/\n\u251c\u2500\u2500 .tigerflow/\n\u2514\u2500\u2500 transcribe/\n    \u251c\u2500\u2500 1.txt\n    \u251c\u2500\u2500 2.txt\n    \u2514\u2500\u2500 ...\n</code></pre> <p>where <code>.tigerflow/</code> is an internal directory storing the pipeline's operational state and related metadata.</p> <p>Warning</p> <p><code>.tigerflow/</code> is what enables resuming a previous pipeline run, so it should not be deleted or modified.</p>"},{"location":"guides/pipeline/#checking-progress","title":"Checking Progress","text":"<p>We can check the pipeline's progress at any point by running:</p> CommandOutput <pre><code>tigerflow report progress path/to/results/\n</code></pre> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Task       \u2503 Processed \u2503 Ongoing \u2503 Failed \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 transcribe \u2502        91 \u2502       0 \u2502      0 \u2502\n\u2502 embed      \u2502        83 \u2502       0 \u2502      8 \u2502\n\u2502 ingest     \u2502        83 \u2502       0 \u2502      0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 91/91 (100.0%)\n</code></pre> <p>where <code>path/to/results/</code> must be a valid output directory containing <code>.tigerflow/</code>.</p>"},{"location":"guides/pipeline/#checking-errors","title":"Checking Errors","text":"<p>If the progress reports any failed files, we can identify them by running:</p> CommandOutput <pre><code>tigerflow report errors path/to/results/\n</code></pre> <pre><code>[embed] 8 failed files (open to view errors):\n  results/.tigerflow/embed/7501863358941940997.err\n  results/.tigerflow/embed/7501867598829702430.err\n  results/.tigerflow/embed/7501869468910423326.err\n  results/.tigerflow/embed/7501869707121757470.err\n  results/.tigerflow/embed/7501870655906860306.err\n  results/.tigerflow/embed/7501870694288985390.err\n  results/.tigerflow/embed/7501870878855154987.err\n  results/.tigerflow/embed/7501870943883545899.err\n</code></pre> <p>Each error file contains specific error messages that help identify and resolve issues in the code or data.</p> <p>Example</p> <p>In this case, all error files contain the same message:</p> <pre><code>Traceback (most recent call last):\nFile \"/home/sp8538/.conda/envs/tiktok/lib/python3.12/site-packages/tigerflow/tasks/local_async.py\", line 47, in task\n    await self.run(self._context, input_file, temp_file)\nFile \"/home/sp8538/tiktok/pipeline/tigerflow/demo/code/embed.py\", line 35, in run\n    resp.raise_for_status()  # Raise error if unsuccessful\n    ^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/sp8538/.conda/envs/tiktok/lib/python3.12/site-packages/aiohttp/client_reqrep.py\", line 629, in raise_for_status\n    raise ClientResponseError(\naiohttp.client_exceptions.ClientResponseError: 400, message='Bad Request', url='https://api.voyageai.com/v1/embeddings'\n</code></pre> <p>which suggests an issue with the embedding API request. However, since the same request was successful for other files, the issue likely lies in the input data (i.e., transcription).</p> <p>Upon inspection, we find the failed files have empty transcriptions, which explains the API request failure. Furthermore, we can confirm that the corresponding videos contain no audio, which led to the empty transcriptions in the first place.</p> <p>We may then exclude such videos from the pipeline to prevent future errors.</p> <ol> <li> <p>TigerFlow uses the dependency information specified in the pipeline configuration to automatically organize input and output directories between tasks, so users do not need to handle this file organization manually.\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/task/","title":"Tasks","text":"<p>In TigerFlow, a task represents a unit of work to be applied to individual files, e.g., transcribing an audio file. Tasks are user-defined Python modules that subclass one of several builtin abstract tasks. These abstract classes provide methods that automatically convert user modules into \"task servers\". Run individually, a task server can be used to bulk process files. Connected, tasks transform into a data processing pipeline.</p> <p>Note</p> <p>TigerFlow tasks are designed for embarrassingly parallel, one-to-one file processing, where each input file is transformed into a single output file independently of all other input files.</p> <p>TigerFlow supports three types of tasks:</p> <ul> <li><code>LocalTask</code>: Runs synchronous operations on a login/head node</li> <li><code>LocalAsyncTask</code>: Runs asynchronous operations on a login/head node</li> <li><code>SlurmTask</code>: Runs parallel operations across compute nodes via Slurm</li> </ul>"},{"location":"guides/task/#hello-tasks","title":"Hello, Tasks!","text":"<p>To define a task, subclass one of the above abstract types and implement the following methods:</p> Method Required Description <code>setup</code> No Initializes shared context used across multiple files (e.g., loading a model) <code>run</code> Yes Contains the processing logic applied to each file <code>teardown</code> No Performs cleanup operations for graceful shutdown (e.g., closing a database connection) <p>Then, simply call the inherited <code>cli()</code> method to turn the module into a runnable CLI application.</p> <p>For instance, here's a simple \"Hello, World!\" local task:</p> hello.py<pre><code>from tigerflow.tasks import LocalTask\n\n\nclass HelloWorld(LocalTask):\n    @staticmethod\n    def setup(context):\n        context.greeting = \"Hello\"\n        print(\"Setup executed successfully!\")\n\n    @staticmethod\n    def run(context, input_file, output_file):\n        with open(input_file, \"r\") as fi:\n            content = fi.read()\n\n        new_content = context.greeting + \", \" + content.upper()\n\n        with open(output_file, \"w\") as fo:\n            fo.write(new_content)\n\n    @staticmethod\n    def teardown(context):\n        print(\"Teardown executed successfully!\")\n\n\nHelloWorld.cli()\n</code></pre> <p>where:</p> <ul> <li><code>context</code> is a namespace to store and access any common, reusable data/objects (e.g., DB connection)</li> <li><code>input_file</code> is a path to the input file to be processed</li> <li><code>output_file</code> is a path to the output file to be generated</li> </ul> <p>Warning</p> <p>In the <code>run</code> method, <code>context</code> is read-only and will raise an error if modified.</p> <p>With <code>HelloWorld.cli()</code>, this module becomes a runnable CLI application and we can check its details by running:</p> CommandOutput <pre><code>python hello.py --help\n</code></pre> <pre><code>Usage: hello.py [OPTIONS]\n\nRun the task as a CLI application\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-dir         PATH  Input directory to read data [required]        \u2502\n\u2502 *  --input-ext         TEXT  Input file extension [required]                \u2502\n\u2502 *  --output-dir        PATH  Output directory to store results [required]   \u2502\n\u2502 *  --output-ext        TEXT  Output file extension [required]               \u2502\n\u2502    --help                    Show this message and exit.                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>We can then run the task as follows:</p> CommandOutput <pre><code>python hello.py --input-dir path/to/data/ --input-ext .txt --output-dir path/to/results/ --output-ext .txt\n</code></pre> <pre><code>2025-09-11 10:54:23 | INFO     | Setting up task\nSetup executed successfully!\n2025-09-11 10:54:23 | INFO     | Task setup complete\n2025-09-11 10:54:23 | INFO     | Starting processing: 5.txt\n2025-09-11 10:54:23 | INFO     | Successfully processed: 5.txt\n2025-09-11 10:54:23 | INFO     | Starting processing: 4.txt\n2025-09-11 10:54:23 | INFO     | Successfully processed: 4.txt\n...\n2025-09-11 10:54:23 | INFO     | Starting processing: 1.txt\n2025-09-11 10:54:23 | INFO     | Successfully processed: 1.txt\n^C2025-09-11 10:54:30 | WARNING  | Received signal 2, initiating shutdown\n2025-09-11 10:54:30 | INFO     | Shutting down task\nTeardown executed successfully!\n2025-09-11 10:54:30 | INFO     | Task shutdown complete\n</code></pre> <p>The task processes every <code>.txt</code> file in <code>input-dir</code> and writes the result to <code>output-dir</code> using the same filename stem and the specified output extension (also <code>.txt</code> in this case). For example, <code>path/to/data/4.txt</code> produces <code>path/to/results/4.txt</code>.</p> <p>Error Files</p> <p>If a task encounters an error, TigerFlow generates an error output file, e.g., <code>4.err</code> instead of <code>4.txt</code>. This file contains specific error messages to assist with debugging.</p>"},{"location":"guides/task/#examples","title":"Examples","text":"<p>Say we want to implement a workflow that involves the following tasks:</p> <ol> <li>Transcribe video files using an open-source model (e.g., Whisper)</li> <li>Embed the transcription files using an external API service (e.g., Voyage AI)</li> <li>Ingest the embeddings into a single-writer database (e.g., DuckDB)</li> </ol> <p>We can create each task as shown below.</p> <p>Follow Along</p> <p>You can follow along with the examples using the code and data provided here. Videos have been substituted with audio files due to intellectual property constraints and storage limitations, but the task logic remains otherwise identical.</p>"},{"location":"guides/task/#transcribing-video-files-slurmtask","title":"Transcribing Video Files (<code>SlurmTask</code>)","text":"<p>We implement the transcription step as a Slurm task because it involves compute-intensive work and we want to process files in parallel.</p> transcribe.py<pre><code>import whisper\n\nfrom tigerflow.tasks import SlurmTask\n\n\nclass Transcribe(SlurmTask):\n    @staticmethod\n    def setup(context):\n        context.model = whisper.load_model(\"/home/sp8538/.cache/whisper/medium.pt\")\n        print(\"Model loaded successfully\")\n\n    @staticmethod\n    def run(context, input_file, output_file):\n        result = context.model.transcribe(str(input_file))\n        print(f\"Transcription ran successfully for {input_file}\")\n\n        with open(output_file, \"w\") as f:\n            f.write(result[\"text\"])\n\n\nTranscribe.cli()\n</code></pre> <p>As shown, the task is defined such that:</p> <ul> <li>The model is loaded once during setup and stored in <code>context</code></li> <li>This pre-loaded model is then accessed from <code>context</code> to transcribe each file</li> </ul> <p>Calling <code>Transcribe.cli()</code> turns this module into a runnable CLI application:</p> CommandOutput <pre><code>python transcribe.py --help\n</code></pre> <pre><code>Usage: transcribe.py [OPTIONS]\n\nRun the task as a CLI application\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-dir             PATH     Input directory to read data [required]            \u2502\n\u2502 *  --input-ext             TEXT     Input file extension [required]                    \u2502\n\u2502 *  --output-dir            PATH     Output directory to store results [required]       \u2502\n\u2502 *  --output-ext            TEXT     Output file extension [required]                   \u2502\n\u2502 *  --max-workers           INTEGER  Max number of workers for autoscaling [required]   \u2502\n\u2502 *  --cpus                  INTEGER  Number of CPUs per worker [required]               \u2502\n\u2502 *  --memory                TEXT     Memory per worker [required]                       \u2502\n\u2502 *  --time                  TEXT     Wall time per worker [required]                    \u2502\n\u2502    --gpus                  INTEGER  Number of GPUs per worker                          \u2502\n\u2502    --sbatch-option         TEXT     Additional Slurm option for workers (repeatable)   \u2502\n\u2502    --setup-command         TEXT     Shell command to run before the task starts        \u2502\n\u2502                                     (repeatable)                                       \u2502\n\u2502    --task-name             TEXT     Task name [default: Transcribe]                    \u2502\n\u2502    --help                           Show this message and exit.                        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>We can then run the task as follows:</p> CommandOutput <pre><code>python transcribe.py \\\n--input-dir path/to/data/ \\\n--input-ext .mp4 \\\n--output-dir path/to/results/ \\\n--output-ext .txt \\\n--max-workers 3 \\\n--cpus 1 \\\n--memory \"12G\" \\\n--time \"02:00:00\" \\\n--gpus 1 \\\n--sbatch-option \"--mail-user=sp8538@princeton.edu\" \\\n--setup-command \"module purge\" \\\n--setup-command \"module load anaconda3/2024.6\" \\\n--setup-command \"conda activate tiktok\"\n</code></pre> <pre><code>2025-09-16 10:53:44 | INFO     | Submitted task with Slurm job ID 690468\n2025-09-16 10:53:44 | INFO     | Status changed: INACTIVE -&gt; PENDING (Reason: (None))\n2025-09-16 10:54:04 | INFO     | Status changed: PENDING (Reason: (None)) -&gt; ACTIVE (0 workers)\n2025-09-16 10:59:55 | INFO     | Status changed: ACTIVE (0 workers) -&gt; ACTIVE (1 workers)\n2025-09-16 11:00:45 | INFO     | 4 processed files\n2025-09-16 11:00:55 | INFO     | Status changed: ACTIVE (1 workers) -&gt; ACTIVE (3 workers)\n2025-09-16 11:00:55 | INFO     | 1 processed files\n2025-09-16 11:01:05 | INFO     | 6 processed files\n...\n2025-09-16 11:03:08 | INFO     | 2 processed files\n2025-09-16 11:04:08 | INFO     | 1 processed files\n2025-09-16 11:04:58 | INFO     | Status changed: ACTIVE (3 workers) -&gt; ACTIVE (1 workers)\n2025-09-16 11:05:58 | INFO     | Status changed: ACTIVE (1 workers) -&gt; ACTIVE (0 workers)\n^C2025-09-16 11:06:40 | WARNING  | Received signal 2, initiating shutdown\n2025-09-16 11:06:40 | INFO     | Shutting down task\n2025-09-16 11:06:40 | ERROR    | Status changed: ACTIVE (0 workers) -&gt; INACTIVE (Reason: CANCELLED+)\n2025-09-16 11:06:41 | INFO     | Task shutdown complete\n</code></pre> <p>The resources specified above, including <code>time</code>, apply to each individual worker. Workers can be spun up and down dynamically in response to incoming workloads, so it is recommended to allocate only the minimal necessary resources per worker. For example, setting the worker <code>time</code> to a smaller value like 2 hours (instead of 12 hours) can reduce scheduling delays, as longer Slurm job requests often result in longer queue times.</p>"},{"location":"guides/task/#embedding-text-files-localasynctask","title":"Embedding Text Files (<code>LocalAsyncTask</code>)","text":"<p>We implement the embedding step as a local asynchronous task because it involves I/O-bound work (i.e., making external API requests) that can be performed concurrently.</p> embed.py<pre><code>import asyncio\nimport os\n\nimport aiofiles\nimport aiohttp\n\nfrom tigerflow.tasks import LocalAsyncTask\n\n\nclass Embed(LocalAsyncTask):\n    @staticmethod\n    async def setup(context):\n        context.url = \"https://api.voyageai.com/v1/embeddings\"\n        context.headers = {\n            \"Authorization\": f\"Bearer {os.environ['VOYAGE_API_KEY']}\",\n            \"Content-Type\": \"application/json\",\n        }\n        context.session = aiohttp.ClientSession()\n        print(\"Session created successfully!\")\n\n    @staticmethod\n    async def run(context, input_file, output_file):\n        async with aiofiles.open(input_file, \"r\") as f:\n            text = await f.read()\n\n        async with context.session.post(\n            context.url,\n            headers=context.headers,\n            json={\n                \"input\": text.strip(),\n                \"model\": \"voyage-3.5\",\n                \"input_type\": \"document\",\n            },\n        ) as resp:\n            resp.raise_for_status()  # Raise error if unsuccessful\n            result = await resp.text()  # Raw JSON\n            await asyncio.sleep(1)  # For API rate limit\n\n        async with aiofiles.open(output_file, \"w\") as f:\n            await f.write(result)\n\n    @staticmethod\n    async def teardown(context):\n        await context.session.close()\n        print(\"Session closed successfully!\")\n\n\nEmbed.cli()\n</code></pre> <p>As shown, the task is defined such that it:</p> <ul> <li>Initializes reusable resources (e.g., HTTP session) and stores them in <code>context</code></li> <li>Utilizes these resources from <code>context</code> to send a request to the external API for each input file</li> <li>Cleans up resources (e.g., HTTP session) at the end to ensure a graceful shutdown</li> </ul> <p>Notice that <code>LocalAsyncTask</code> requires all operations to adhere to Python's <code>async</code>/<code>await</code> syntax. For example, file reading and writing should be performed using <code>aiofiles</code>, since standard file I/O would block the event loop and prevent concurrent file processing. Similarly, <code>LocalAsyncTask</code> should not include compute-intensive logic, as this would also block the event loop and goes against its intended use. For compute-heavy tasks, consider using <code>SlurmTask</code> instead.</p> API Rate Limits <p>API services often enforce rate limits (e.g., 2000 requests per minute). To comply with these limits, we can use <code>asyncio.sleep()</code> within the <code>run</code> logic (as shown above), in combination with the <code>--concurrency-limit</code> option (see below), which controls the maximum number of files processed concurrently.</p> <p>For example, if each API request takes less than a second and the service allows up to 2000 requests per minute, we can:</p> <ul> <li>Use <code>asyncio.sleep(1)</code> in the <code>run</code> logic to ensure each request takes at least one second</li> <li>Set <code>--concurrency-limit</code> to 30 to ensure no more than 30 requests are processed concurrently</li> </ul> <p>Together, these measures effectively cap the request rate at 1800 requests per minute, keeping it safely within the limit.</p> <p>Calling <code>Embed.cli()</code> turns this module into a runnable CLI application:</p> CommandOutput <pre><code>python embed.py --help\n</code></pre> <pre><code>Usage: embed.py [OPTIONS]\n\nRun the task as a CLI application\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-dir                PATH     Input directory to read data [required]                                 \u2502\n\u2502 *  --input-ext                TEXT     Input file extension [required]                                         \u2502\n\u2502 *  --output-dir               PATH     Output directory to store results [required]                            \u2502\n\u2502 *  --output-ext               TEXT     Output file extension [required]                                        \u2502\n\u2502 *  --concurrency-limit        INTEGER  Maximum number of coroutines that may run concurrently at any given     \u2502\n\u2502                                        time (excess coroutines are queued until capacity becomes available)    \u2502\n\u2502                                        [required]                                                              \u2502\n\u2502    --task-name                TEXT     Task name [default: Embed]                                              \u2502\n\u2502    --help                              Show this message and exit.                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>We can then run the task as follows:</p> CommandOutput <pre><code>python embed.py \\\n--input-dir path/to/data/ \\\n--input-ext .txt \\\n--output-dir path/to/results/ \\\n--output-ext .json \\\n--concurrency-limit 30\n</code></pre> <pre><code>2025-09-19 10:28:32 | INFO     | Setting up task\nSession created successfully!\n2025-09-19 10:28:32 | INFO     | Task setup complete\n2025-09-19 10:28:32 | INFO     | Starting processing: 7501870786337115434.txt\n2025-09-19 10:28:32 | INFO     | Starting processing: 7501870786941127967.txt\n2025-09-19 10:28:32 | INFO     | Starting processing: 7501870783862541576.txt\n...\n2025-09-19 10:28:34 | INFO     | Starting processing: 7501869901028592927.txt\n2025-09-19 10:28:34 | INFO     | Successfully processed: 7501871089715268906.txt\n2025-09-19 10:28:34 | INFO     | Starting processing: 7501870443775692078.txt\n2025-09-19 10:28:34 | INFO     | Successfully processed: 7501863546456771870.txt\n2025-09-19 10:28:34 | INFO     | Starting processing: 7501869899782901022.txt\n2025-09-19 10:28:34 | INFO     | Successfully processed: 7501870775461317906.txt\n...\n2025-09-19 10:28:36 | INFO     | Successfully processed: 7501870542656474398.txt\n2025-09-19 10:28:36 | INFO     | Successfully processed: 7501870861306318126.txt\n2025-09-19 10:28:36 | INFO     | Successfully processed: 7501870700089707807.txt\n^C2025-09-19 10:28:40 | WARNING  | Received signal 2, initiating shutdown\n2025-09-19 10:28:40 | INFO     | Shutting down task\nSession closed successfully!\n2025-09-19 10:28:40 | INFO     | Task shutdown complete\n</code></pre>"},{"location":"guides/task/#ingesting-text-embeddings-localtask","title":"Ingesting Text Embeddings (<code>LocalTask</code>)","text":"<p>We implement the ingestion step as a local synchronous task because our target database (DuckDB) only supports writes from a single process.</p> ingest.py<pre><code>import json\n\nimport duckdb\n\nfrom tigerflow.tasks import LocalTask\n\n\nclass Ingest(LocalTask):\n    @staticmethod\n    def setup(context):\n        db_path = \"/home/sp8538/tiktok/pipeline/tigerflow/demo/results/test.db\"\n\n        conn = duckdb.connect(db_path)  # Creates file if not existing\n        print(f\"Successfully connected to {db_path}\")\n\n        conn.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS embeddings (\n                id UBIGINT,\n                embedding FLOAT[1024],\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n            );\n        \"\"\")\n\n        context.conn = conn\n\n    @staticmethod\n    def run(context, input_file, output_file):\n        with open(input_file, \"r\") as f:\n            content = json.load(f)\n\n        embedding = content[\"data\"][0][\"embedding\"]\n\n        context.conn.execute(\n            \"INSERT INTO embeddings (id, embedding) VALUES (?, ?)\",\n            (input_file.stem, embedding),\n        )\n\n    @staticmethod\n    def teardown(context):\n        context.conn.close()\n        print(\"DB connection closed\")\n\n\nIngest.cli()\n</code></pre> <p>As shown, the task is defined such that:</p> <ul> <li>A database connection is created once during setup and stored in <code>context</code></li> <li>This database connection is then accessed from <code>context</code> to ingest each file</li> <li>The database connection is closed at the end to ensure a graceful shutdown</li> </ul> <p>Calling <code>Ingest.cli()</code> turns this module into a runnable CLI application:</p> CommandOutput <pre><code>python ingest.py --help\n</code></pre> <pre><code>Usage: ingest.py [OPTIONS]\n\nRun the task as a CLI application\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --input-dir         PATH  Input directory to read data [required]         \u2502\n\u2502 *  --input-ext         TEXT  Input file extension [required]                 \u2502\n\u2502 *  --output-dir        PATH  Output directory to store results [required]    \u2502\n\u2502 *  --output-ext        TEXT  Output file extension [required]                \u2502\n\u2502    --task-name         TEXT  Task name [default: Ingest]                     \u2502\n\u2502    --help                    Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>We can then run the task as follows:</p> CommandOutput <pre><code>python ingest.py \\\n--input-dir path/to/data/ \\\n--input-ext .json \\\n--output-dir path/to/results/ \\\n--output-ext .out\n</code></pre> <pre><code>2025-09-19 13:02:16 | INFO     | Setting up task\nSuccessfully connected to /home/sp8538/tiktok/pipeline/tigerflow/demo/results/test.db\n2025-09-19 13:02:16 | INFO     | Task setup complete\n2025-09-19 13:02:16 | INFO     | Starting processing: 7501869531975929119.json\n2025-09-19 13:02:17 | INFO     | Successfully processed: 7501869531975929119.json\n2025-09-19 13:02:17 | INFO     | Starting processing: 7501869470705782062.json\n2025-09-19 13:02:17 | INFO     | Successfully processed: 7501869470705782062.json\n2025-09-19 13:02:17 | INFO     | Starting processing: 7501871439457373470.json\n...\n2025-09-19 13:02:18 | INFO     | Successfully processed: 7501870861306318126.json\n2025-09-19 13:02:18 | INFO     | Starting processing: 7501870700089707807.json\n2025-09-19 13:02:18 | INFO     | Successfully processed: 7501870700089707807.json\n^C2025-09-19 13:02:22 | WARNING  | Received signal 2, initiating shutdown\n2025-09-19 13:02:22 | INFO     | Shutting down task\nDB connection closed\n2025-09-19 13:02:22 | INFO     | Task shutdown complete\n</code></pre> <p>Output Files</p> <p>Note that we specify <code>--output-dir</code> and <code>--output-ext</code> even though the task\u2019s <code>run</code> logic does not write to <code>output_file</code>. This is necessary because TigerFlow creates an empty \"placeholder\" file even when no content is written. This placeholder indicates that the file was processed successfully according to the user-provided <code>run</code> logic, even if no concrete output was produced.</p>"}]}